Single Author info:

btadiko Bapiraju Vamsi Tadikonda


## How to Run the Job ##
* Prerequisites - Have spark and hadoop setup on the server and has java installed.
* Compile the java file TFICF.java and save the classes as a jar file.
javac TFICF.java
jar cf TFICF.jar TFICF*.class

* Run the jar file using "spark-submit" command and store the result in a text file.
spark-submit --class TFICF TFICF.jar input &> spark_output.txt
grep -v '^2022\|^(\|^-' spark_output.txt > output.txt

* Compare the output with the solution provided
diff -s solution.txt output.txt


## Overview of the Code ##
    #1 TF Job 
         * TF Job (Word Count Job + Document Size Job)
         * Gathers all data needed for TF calculation from wordsRDD
         * Input:  ( (word@document) , docSize )
         * Map:    ( (word@document) , (1/docSize) )
         * Reduce: ( (word@document) , (wordCount/docSize) )
    --> Inorder to Map the all the tuples, The program uses mapToPair spark function with just a small string manipulation to the second element in the input tuples(x).
        x -> new Tuple2<String,String>(x._1, "1/"+Integer.toString(x._2)) 
    --> In the Reduce phase, The program uses reduceByKey function which internally calls Function2 function provided by Spark and overrides the call function. In the call function, The docSize is extracted and stored while the total count of a key is calculated by adding the individual counts of each value's numerator.
    
    #2 ICF Job 
         * Gathers all data needed for ICF calculation from tfRDD
         * Input:  ( (word@document) , (wordCount/docSize) )
         * Map:    ( word , (1/document) )
         * Reduce: ( word , (numDocsWithWord/document1,document2...) )
         * Map:    ( (word@document) , (numDocs/numDocsWithWord) )

    --> The Program maps the current PairRDD's to the requested format by returning a new Tuple2 object with splitting the word and document present in the input key.
    --> The PairRDD's are then reduced by concatenating all the document names in the denominator and incrementing the numDocsWithWord variable. The reduction is done using reduceByKey Spark function along with overridding the call function inside the Function2.
    --> After the reduced PairRDDs, the program uses flatMapToPair function (like in the initial Job) to split each key-value pair containing the word and list of documents to have key-value pairs with word and only a single document. Inorder to this, The program collects required attributes(word,numDocsWithWord, list of documents) from the input and Maintains an array to save all the individual key-value pairs.

    #3 TF-ICF Job
         * Calculates final TFICF value from tfRDD and icfRDD
         * 3.1
         * Input:  ( (word@document) , (wordCount/docSize) )          [from tfRDD]
         * Map:    ( (word@document) , TF )
         * 3.2
         * Input:  ( (word@document) , (numDocs/numDocsWithWord) )    [from icfRDD]
         * Map:    ( (word@document) , ICF )
         * 3.3
         * Union:  ( (word@document) , TF )  U  ( (word@document) , ICF )
         * Reduce: ( (word@document) , TFICF )
         * Map:    ( (document@word) , TFICF )

        --> 3.1) The part was already implemented which calculates the value of WordCount/docSize for each word and Document combination.
            
            TF = wordCount/docSize

        --> 3.2) This part is similar to the part in 3.1. The program parses the input value to fetch numDocs and numDocsWithWord. It then calculates the ICF value using the formula below and returns it.

            ICF   = log( (Total numDocs in the corpus + 1) / (numDocsWithWord in the corpus + 1) )

        --> 3.3) In this part, The program first combines the pairRDDs from the 3.1 and 3.2 using the union function and then applies mapToPair to calculate the TF-ICF. 
        NOTE: The template file which provided 3.1 forgot to include log10 value as mentioned in the paper. Incorporating it while calculating the TFICF. So While calculating the TFICF, log10 of the values in 3.1 was computed.

        So the equation became:  Math.log10(TF +1.0)*ICF;

        --> Finally, The key in each PairRDD was renamed to have the document name first and the word after that. Inorder to do that the program used another mapToPair function call.


